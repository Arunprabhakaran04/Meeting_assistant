{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cTrG93KWt-n0lITRH4TiuEURmYyNmG-U",
      "authorship_tag": "ABX9TyOgtNkmLWffmTvaUiCs35sE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arunprabhakaran04/Meeting_assistant/blob/main/Meeting_assistant_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "iYIbBTae-QMh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9OuMIs09r-V",
        "outputId": "e46bc425-a43f-4fc4-e55e-cbbc079262dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install transformers\n",
        "!pip -q install google-search-results\n",
        "!pip -q install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain faiss-cpu langchain-huggingface PyPDF2 langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWkSxvsg-AfF",
        "outputId": "a2e49b08-4d09-438a-a923-ba0ab30b64dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.5/411.5 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from serpapi import GoogleSearch\n",
        "import os\n",
        "from IPython.display import display, Audio, clear_output"
      ],
      "metadata": {
        "id": "ZKozJzLy-ESJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPLZJifw-H8W",
        "outputId": "2a200558-e2c1-4c19-9816-6afa277a088c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import re\n",
        "import requests"
      ],
      "metadata": {
        "id": "o8_w_omT-KPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Groq-Whisper Model"
      ],
      "metadata": {
        "id": "TccjUr4D-Tzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get(\"stt_bot_groq\")"
      ],
      "metadata": {
        "id": "h5y2Ut1x-W3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio_with_groq(audio_path):\n",
        "    if not groq_api_key:\n",
        "        raise ValueError(\"GROQ_API_KEY not set. Please set it as an environment variable.\")\n",
        "\n",
        "    api_url = \"https://api.groq.com/openai/v1/audio/transcriptions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {groq_api_key}\"\n",
        "    }\n",
        "\n",
        "    files = {\n",
        "        \"file\": (os.path.basename(audio_path), open(audio_path, \"rb\"), \"audio/m4a\")\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"whisper-large-v3\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, headers=headers, files=files, data=data)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    transcription_data = response.json()\n",
        "    transcription = transcription_data.get(\"text\", \"\").strip()\n",
        "\n",
        "    return transcription"
      ],
      "metadata": {
        "id": "J6g7z4ch-hMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question Extraction"
      ],
      "metadata": {
        "id": "XgYRTrcS-jYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "extract_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=128\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvRAX6Qu-oIo",
        "outputId": "6cc37575-fb10-4653-86a3-200743dd414b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-b6f9d4e8bec3>:3: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  extract_llm = HuggingFaceEndpoint(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_question(transcription):\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful assistant. Your goal is to identify the main question asked by the user in the conversation below,\n",
        "    even if the user does not explicitly end with a question mark. Consider the user's intent, and if the user is asking for\n",
        "    information, clarification, or how to do something, extract that question. If no question is asked, return exactly\n",
        "    \"No clear question found.\"\n",
        "\n",
        "    Return only the user's question in a natural, human-readable sentence (as a question), and nothing else.\n",
        "\n",
        "    Conversation:\n",
        "    \\\"\\\"\\\"{transcription}\\\"\\\"\\\".\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = extract_llm(prompt).strip()\n",
        "        if \"No clear question found\" in response:\n",
        "            return \"No clear question found.\"\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error in processing: {str(e)}\""
      ],
      "metadata": {
        "id": "L61ZO8KG-rNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Classification"
      ],
      "metadata": {
        "id": "SPVwaNjY-s0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_action(question):\n",
        "    print(\"Classifying question using Zero-Shot Classification model...\")\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "    candidate_labels = [\"RAG\", \"Web Search\", \"LLM Answer\"]\n",
        "\n",
        "    prompt_template = (\n",
        "        \"You are an expert AI assistant trained to classify user queries into specific categories for a decision-making system.\\n\"\n",
        "        \"There are three categories to choose from:\\n\"\n",
        "        \"1. 'RAG' (Retrieval-Augmented Generation): Use this if the question requires retrieving detailed or structured knowledge from a database, document, or predefined dataset or about any project that the user has worked on.\\n\"\n",
        "        \"2. 'Web Search': Use this if the question is about real-time events, external facts, or knowledge not available in local datasets. Web search should only be selected for time-sensitive, factual, or real-world inquiries.\\n\"\n",
        "        \"3. 'LLM Answer': Use this if the question can be answered purely by a language model's general knowledge, reasoning capabilities, or linguistic understanding without external data.\\n\\n\"\n",
        "        \"Guidelines:\\n\"\n",
        "        \"- Analyze the question's intent and content thoroughly before deciding.\\n\"\n",
        "        \"- If the query mentions current news, live updates, or external information, classify it as 'Web Search'.\\n\"\n",
        "        \"- If the query can be answered with general knowledge, logical reasoning, or basic facts, choose 'LLM Answer'.\\n\"\n",
        "        \"- If the query deals about personal project/interest or requires deep or structured knowledge not immediately available, classify it as 'RAG'.\\n\"\n",
        "        \"- Return only one category: RAG, Web Search, or LLM Answer.\\n\"\n",
        "        \"- Ensure the classification is precise and relevant.\\n\\n\"\n",
        "        f\"User Query: {question}\"\n",
        "    )\n",
        "    result = classifier(\n",
        "        prompt_template,\n",
        "        candidate_labels=candidate_labels,\n",
        "        hypothesis_template=\"The query falls under {}.\"\n",
        "    )\n",
        "    classification = result['labels'][0]\n",
        "    return classification"
      ],
      "metadata": {
        "id": "hMLLHAoX-v0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test1"
      ],
      "metadata": {
        "id": "zhnehAJt-y3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_use_case():\n",
        "    print(\"--- Sample Use Case: Classification Task ---\")\n",
        "    # Mocked transcription without a question mark, resembling voice output\n",
        "    mock_transcription = 'hi arun! hope you are doing well. this is in regard to your application for the role of gen ai developer. tell me more about your RAG project '\n",
        "    print(\"Transcription:\", mock_transcription)\n",
        "\n",
        "    # Extract Question\n",
        "    question = extract_question(mock_transcription)\n",
        "    print(\"Extracted Question:\", question)\n",
        "\n",
        "    if question not in [\"No clear question found.\", \"Error in processing\"]:\n",
        "        # Classify Action\n",
        "        classification = classify_action(question)\n",
        "        print(\"Action Decided:\", classification)\n",
        "    else:\n",
        "        print(\"No question extracted or an error occurred.\")"
      ],
      "metadata": {
        "id": "AWdZ6val-1Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_use_case()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611,
          "referenced_widgets": [
            "36ab2e06af7c4a10a387fb38e9664c01",
            "16bcaeb650114168afca48d0e4cc7200",
            "a8675bbaf6a1438b8a3cec46b419a03b",
            "5331275727954a4b94f1a696357cfe5b",
            "64ee5ecbd75e47e59be3db50df6e844c",
            "3c5734e344d0410aabcae09621cf94fb",
            "7fd24da5ee944ab897c91c9879519ad9",
            "46cb63634fdd4cb78075a5f794287381",
            "cd19c35eb90d46b38b606f7fe1f0a24d",
            "4792868df9e94e97b90afe554656cfaa",
            "358fa5774e5444ba89d5c051479b8f69",
            "b52e6d69c15f48248f19285a11285fb6",
            "9e72ab8b59a7438797ac4e58d126d335",
            "24d3fc023e034643a296a039a7acd2b5",
            "53e9b7b7e1b248cf8e330b24ee71890e",
            "414be2c9f8ec487c8a382d0c40c6f08f",
            "2f44d9da3e2440a7ae20520ab6139d37",
            "93ebf22cec7344c3a55ec7149ca4b804",
            "f9eb04050c1c42a3a42967a2ef7af077",
            "5cc8f631385c46f7bbfda6bffaed27bf",
            "3239e99b4d1747b784d052dafa0c6347",
            "41b75bcc48134545b97de086bbc3adf5",
            "992879b37e69417e9591b28cf6bb921f",
            "2cc2b8b409474d1889ad18b816c362b1",
            "8975e422bf8b4ba4ab7ec1ade7fb2884",
            "9274e65df34a4c14b60fe7372ac6f51f",
            "4aebee9ddfbc4571887f905b2bb6f850",
            "7b86f49a7e564b1ea26fe97ca8f280e5",
            "ed2589bfe9c842cdb9f051160dce0e88",
            "ecaf3dc4809f479db8587e15c4ef5e9b",
            "b742bcd917eb465d94516c2b81874b08",
            "785825430eca4a78ae4d90b951da1dc5",
            "0c5049e38a32477c928ffbdebc55b038",
            "c08ecb6f09f047b8a7518dc4ba43ee7f",
            "1ca2a7d0d6c7497fa1a860449da32a03",
            "6fc224baa6944867aa348218668ce18f",
            "ebf1533360bd4bf0b008ba2f0ac50e03",
            "5205d80e8a2c4c0081dc58ad344e715c",
            "1a43c0de84dd4488ae1714c03b51f640",
            "f94232d1643347d4864cbbeb3600e5c0",
            "7d6798f0594845da891819fec9d55c83",
            "c52bb25f21774c998691c5a4ca3c4486",
            "875603fafe70493e95258b4939036ecf",
            "184f800e1844428bb335dc05f5ea37ba",
            "566138c1484141ae8355ba6fce87e08e",
            "d5779f3d478e452eb847630b07ee221a",
            "fb99392bdce2418593a5d4d3645854aa",
            "3b3e417faac748b9b7a1a1734897cc91",
            "d65cecdade014c4f843b5f6a9f16d0a5",
            "9238f6e4b4714739849394761003bfee",
            "1ac6d0ca5af1405aa7ab745f0fd4d168",
            "16aa8ede135e4781b63a8dff1f78f1a2",
            "140c4ca075804e9fb0ac38efdfcf2426",
            "894f80d9320e4e39b44930b1a752e58b",
            "22d7289ea116422aaaf0827c5a3bbd94",
            "d9c20112d9294191ae52f570f90971cb",
            "6c1c02efbb9d48288aba264d60fea100",
            "4735e82c078d48509de7cda1704ba53a",
            "62a9aaf188cf4d59885865de947e89f6",
            "fcdca84c353c418488c8df54ea20b3a5",
            "101745744a32438e9445c9d4fd9a5af2",
            "8eb189241547475f9b1567ec62c30874",
            "b97a19e48d74440f8f813c5e6a83821b",
            "023d59a805904cce97850cf87f789893",
            "d53c679d9bed4420a94e40816d579069",
            "b3332032b17543d6bdd08240cf6cc58d"
          ]
        },
        "id": "rD2XKKdz-2m8",
        "outputId": "dcece15d-4e99-489c-a0e5-484a8cd108c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sample Use Case: Classification Task ---\n",
            "Transcription: hi arun! hope you are doing well. this is in regard to your application for the role of gen ai developer. tell me more about your RAG project \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-8b6f417e3b68>:15: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = extract_llm(prompt).strip()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Question: \"\"\"\n",
            "\n",
            "    return \"What is your RAG project about?\"\n",
            "\n",
            "def main(request):\n",
            "    if request.method == \"POST\":\n",
            "        form = RAGForm(request.POST)\n",
            "        if form.is_valid():\n",
            "            form.save()\n",
            "            return redirect(\"success\")\n",
            "    else:\n",
            "        form = RAGForm()\n",
            "    return render(request, \"rag_form.html\", {\"form\": form})\n",
            "\n",
            "The issue with the code is that the `return` statement is not inside the `if` block. It should be inside the `if` block\n",
            "Classifying question using Zero-Shot Classification model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36ab2e06af7c4a10a387fb38e9664c01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b52e6d69c15f48248f19285a11285fb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "992879b37e69417e9591b28cf6bb921f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c08ecb6f09f047b8a7518dc4ba43ee7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "566138c1484141ae8355ba6fce87e08e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9c20112d9294191ae52f570f90971cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Decided: RAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building Custom Agents"
      ],
      "metadata": {
        "id": "OMRZNxzi-4GM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Agent 1: RAG"
      ],
      "metadata": {
        "id": "8s6TrnYk_Oqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Colab Notebooks/MeetingBot/Arun_Resume.pdf\""
      ],
      "metadata": {
        "id": "cuO7v4OI_ROo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "sec_key = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = sec_key\n"
      ],
      "metadata": {
        "id": "npEcMUXt_Sh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "P6Hzpdty_Trn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_reader = PdfReader(path)"
      ],
      "metadata": {
        "id": "mZFX5vIZ_VGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(document_reader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBDQxXgwNXVF",
        "outputId": "9c9bc051-56a3-4dc3-9feb-8381a1950934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PyPDF2._reader.PdfReader object at 0x7b8ad8d0f8e0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading data from the pages and adding it to a raw string\n",
        "raw_text = \"\"\n",
        "for i, pages in enumerate(document_reader.pages):\n",
        "  text = pages.extract_text()\n",
        "  if text:\n",
        "    raw_text += text\n",
        "\n"
      ],
      "metadata": {
        "id": "SHtHMWjmNl0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWZxuyhdNoGh",
        "outputId": "7a11e9a5-7899-4acd-b6e6-1d746cf17636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3160"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "n-xN0M_yNpzq",
        "outputId": "0785080e-09b0-44a2-c2a1-5ff1fac6741b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Arun\\nPrabhakaran\\nChennai,\\nTamil\\nNadu,\\nIndia\\n|\\n+91-\\n9841633771\\n|\\nLinkedIn\\n|\\nGitHub\\n|\\narunprabhakaran1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size= 1000,\n",
        "    chunk_overlap = 200,\n",
        "    length_function = len\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "xsA5NqD9NrKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts), type(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7QwlhMZNs0v",
        "outputId": "8acea0dd-beed-4d68-a7de-b1988d386798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, list)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424,
          "referenced_widgets": [
            "a3749159f9044329b72f2caedfed6810",
            "aec054db90ef49ee8de935fb7a7c4b9e",
            "3e19ea2895b24496867d411f4746ae5c",
            "1641da5492894de4b2bff1b79a8b3c12",
            "c802bc88e1d840a69921112b792cf7f7",
            "8fabae7a42d74a12a9264cde900d5b87",
            "c7b027fa566843a6884329c91045576f",
            "c9b967881b074c628d0e8edbbb2e5de0",
            "e72a6b368db04eccadf22322baa9e2b8",
            "c7500c016ac5423ca52eac6d15ab99bb",
            "68778692db4d43f58370c4fef01769e1",
            "0928aede11c5490aa228fc3259d916da",
            "9bb18abd58a141fa9a14ae3a233b4629",
            "0b02027b400748988ffb46e76246498f",
            "9c4dbe975ad44cdcb3429a50b2f8e26a",
            "87166e58ff7749218767389762c092d3",
            "c9d65752f1874498813f5ee31717889c",
            "c746eb3049a94c9c86eeedb82ba1cba1",
            "a40b6267f6824af19b72d8eee6b1c4e9",
            "8ce9bba5c9d14322ad23da414da29762",
            "76bc595187304a9b9a0a8f671be8039a",
            "e0ae5b00212d44bfad689764d86f5fa5",
            "b5725dcd13c449feba29da5aed7d77d9",
            "3eba0c8b97fd49a1831cb69d042096a0",
            "cb0c758fa2934e928aa96ba0d2652e84",
            "873806c38d6445b7947239d8fcbb5504",
            "c27fd82ac6f04cbb81de6c64ff51a2b1",
            "bb9d46a41c0c4ccba7c353dd8038ff96",
            "9f42460a57144d15a7fddf9b7930d0ee",
            "1b937a0924804faa947c2f15e85004cd",
            "7257767f88fe4fbbb03f9f0058fc4603",
            "f79e3cf9268d40f79a65b0bf2043b8b3",
            "7620e1ff1311400ba357976424a4f5cc",
            "7925de1af6294d51a5a9322a734a05cc",
            "16c23180fcf74bf199b754bb026a22c2",
            "943fe13ef34647308beb99ca99f90760",
            "785a960ff0774420a907cd5bfd73b0d1",
            "f84a513ac26d47939ea7f720aeb9e2d4",
            "fea4d3293b9346c1bb90a5cf39cf2f6a",
            "2553e99056444e508d6bd9367befb82e",
            "6ef7d4cb1d5649779982b050e01d43ea",
            "1d47177b3c844b7bb1e36449c6e6317a",
            "c96a3dc1ea5a45fd8af4826c60bd6df3",
            "9495300b810e433fab4035ef24f348c7",
            "fc58dd0002dd45c989f38e273e825dfa",
            "b320d97addee4d2e9d8377c490b1de51",
            "1c274f066e714128bc7c0b169af4179a",
            "c85e9cbf998e472eb434d97b74971b5e",
            "e0d399af9eec4ace8c39eb04ed3292bd",
            "eea78148c756410aa563c1feceb52fb3",
            "bf639db7d4ab483195c45bb273fea897",
            "570bad1222f64ce7b8410c26b1fd1967",
            "91211f61a0374dada8ec619d06723c5e",
            "921cbc0720c94276af6ab90adc8407a0",
            "e8aced0753d945dd9661e645af7b1dd2",
            "9df1f3fd5ac543599b64fd98f1dda278",
            "1197623729ce49f1a71bbc8712c5ee3e",
            "a12d0851905944179462280a53501d82",
            "fdbf3e97d5ba411aa0c987f1199da787",
            "d8a8f42bc76d4d4ba49483cac7888262",
            "c92b3abf704d49fea307582d647f9aeb",
            "f3f9e058ce3940068b1f81778c5403fe",
            "1ced15ee6ad74bacadb2f71c0bcd26b3",
            "70a7173424d045859ae91593927bde4b",
            "5d200e68e32c439d901f9ffd469dac25",
            "b517c76726b64519879b663555d9dddb",
            "c134bec5d8384c2fbb8f933dcb64377a",
            "b820cd1a3bbf48609f839ae5d0080c5c",
            "74810fe290d04466a5969096b6b62a45",
            "9797a94dd0da4ba4bc16f64e3172a5e9",
            "dcc0fd527f194f66b62b112e74a0e9ed",
            "6f659cb828ec436ba30b3ce1301a9492",
            "c5956a1d001d4d9b85349b6ce59e0004",
            "de64eb037c594f9dbecb29bcf56c9c7f",
            "b0e60d66a70342599f3a4edf5c4056dc",
            "54dce8b4fc7c49468d27f136f68f0a9c",
            "2b5b435579f04ed5b0d7ca293a6f9864",
            "55c0dddd2642493080df59c0c861aadd",
            "da925af9979d4e3e920283e0d0aff24d",
            "d10202d046924630b791b36f5f87128c",
            "14fdcad31bd5487caa0eabe1bc6fa932",
            "2e18f5269daf43afa440972c2a5b0968",
            "e7ddc14601234106b698daabb4cb1f99",
            "f03ad462950f4520b737aad93eb53eec",
            "c15c9824abb24fce82309f5f94ebb364",
            "3b158a9d01bd492b8fe85dcd66dea78b",
            "88e3e858b9264ea8a9097ad32901607e",
            "8cf36e3a16d6421ab4932b24f5950f3f",
            "b250aaf4435a443aaaa1f730e56f79ac",
            "f40920b7c3ac460998e0bc40fdaa9d0f",
            "6a83046f2bd3484581afa88fb43938bc",
            "b4bda4849d5c40e7877b0b8c42fb2a58",
            "e95c73c22d804c1ebe6db4e4c26a1f43",
            "2c9dd8454f0d47a3ad855535b8f799a3",
            "2cca162d98d04303a321fc616b8b7d72",
            "193ceb82564a4ec9897778f31a4f522a",
            "fe1fed5415a045b08a52c1f346e8f5ac",
            "177379747dcf42409b584808674a78c1",
            "9fb458b6bffb4159841764f1ed93a9f1",
            "d349cd98a6524a458d1955426ba59c4e",
            "5175f9a9e9974b9b8bfdeef39b8428b5",
            "95948a6129ea4d538166c6044187babd",
            "845b774cc1004f689282eeadc2f0d9c6",
            "d72cd2ca03fa44429761ce2001aecadd",
            "b83db25f06dd4232ac6c57ff2bf6778e",
            "0e6edc9e80274730b1dfe0797daeab1b",
            "d0d982e68ae34348b9d77090d1b640c9",
            "8b00319707ac4509a8afb03f9b8d672b",
            "0928a53925d7454b932c4ffe68349f7a",
            "2897499f20bf4239b6abc2c3095eb4cc",
            "d3dc9f78f0ae4ad9acab9fd263876bd5",
            "f1041a2c29444b5b818b0c666c299af6",
            "83e6569c4e5744ada36f1b2883427124",
            "9c72b8288be042e18ad10bffacd469e9",
            "dc9646acbe9742a0adc475e2b942477d",
            "2f1f233675df47b6bcb61610f484c74d",
            "e4aadd919a1349f7ae5d4d35c6a03f85",
            "45e91064039344788d5efe0c93963b31",
            "afd931e582a744ac9ee64e11407c2dc1",
            "6a5e840b6fa3477290860340c78f6535",
            "96e94f63b24b40248ee14725eac58e7a"
          ]
        },
        "id": "9odggBaeNuTp",
        "outputId": "e0c75cf8-3174-4509-85fa-65eb4981f043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-ad449aa8f6d2>:6: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceBgeEmbeddings(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3749159f9044329b72f2caedfed6810"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0928aede11c5490aa228fc3259d916da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/90.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5725dcd13c449feba29da5aed7d77d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7925de1af6294d51a5a9322a734a05cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc58dd0002dd45c989f38e273e825dfa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9df1f3fd5ac543599b64fd98f1dda278"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c134bec5d8384c2fbb8f933dcb64377a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55c0dddd2642493080df59c0c861aadd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b250aaf4435a443aaaa1f730e56f79ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d349cd98a6524a458d1955426ba59c4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3dc9f78f0ae4ad9acab9fd263876bd5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOAj_E91NxZR",
        "outputId": "127ebb65-ccd0-439a-e386-227291749f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "  (2): Normalize()\n",
              "), model_name='BAAI/bge-small-en', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='', show_progress=False)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_search = FAISS.from_texts(texts, embeddings)\n",
        "doc_search.embedding_function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9woDqO6Nz1Z",
        "outputId": "b39ecb44-30ef-4906-c744-6a687c820ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "  (2): Normalize()\n",
              "), model_name='BAAI/bge-small-en', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='', show_progress=False)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test - retrival"
      ],
      "metadata": {
        "id": "Y7o-QMOoOND5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"acheivement in yantra\"\n",
        "docs = doc_search.similarity_search(query)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9-45aTIQN981",
        "outputId": "d5c5bb4e-c043-4ca7-fb03-c6eed95e7ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='1d133441-d992-48d1-8f74-7f88a5362def', metadata={}, page_content='ML\\nclassification.\\nCo-developing\\nan\\nAI-driven\\nchatbo\\nt\\non\\na\\ncentralized\\nweb\\nplatform\\n,\\nenabling\\nusers \\nto\\nquery\\ncategorized\\ndata\\nand\\nmake\\ninformed\\npurchasing\\ndecisions.\\nCurrently\\nin\\ndevelopment.\\n●\\nSJLT\\nGroup\\nMay’24\\n-\\nJuly’24 \\nAI/ML\\nSpecialist\\n●\\nDesigned\\nand\\ndeveloped\\na\\nmultilingual\\nRAG\\nchatbot\\nfor\\n100+\\ndaily\\nusers\\n,\\nenhancing\\nefficiency\\nby \\naccurately\\nretrieving\\ninformation\\nfrom\\ncompany\\nresources.\\nBuilt\\na\\ndata\\npipeline\\nto\\nstreamline\\nupdates \\nand\\nadded\\na\\nvoice\\nsearch\\nfeature\\nfor\\nimproved\\naccessibility.\\nACHIEVEMENTS\\n●\\nSecured\\n3rd\\nplace\\nin\\nHelios\\nat\\nYantra\\n2024\\nTechnical\\nFest\\n.\\nLed\\na\\nteam\\nin\\nideating\\nsolutions\\nfor\\na\\ndrone\\nto \\nexplore\\na\\ngas\\ngiant.\\nFocused\\non\\nsensor\\nselection,\\nconceptualizing\\ndrone\\nmodels,\\nand\\ndeveloping\\nreinforcement \\nlearning\\nmodels\\n,\\nincluding\\nDeep-Q\\nNetworks\\n,\\nfor\\nanalyzing\\nsensor\\ndata\\nand\\nenabling\\nautonomous\\nnavigation.\\nPROJECTS\\nPDFInsightAI\\nJune’24\\n●\\nDeveloped\\nPDFInsightAI,\\na\\nmultilingual\\nchatbo\\nt\\nutilizing\\nLangchain,\\nHugging\\nFace\\nfor\\nRetrieval\\nAugmented \\nGeneration'),\n",
              " Document(id='04ad6543-8614-4515-b103-19cffabc3d98', metadata={}, page_content='Arun\\nPrabhakaran\\nChennai,\\nTamil\\nNadu,\\nIndia\\n|\\n+91-\\n9841633771\\n|\\nLinkedIn\\n|\\nGitHub\\n|\\narunprabhakaran1729@gmail.com\\nEDUCATION\\n●\\nB.Tech-CSE,\\nVIT\\nVellore\\n(2022-Present):\\n9.57\\nCGPA\\n●\\nClass\\nXII,\\nMaharishi\\nVidya\\nMandir\\n(2022):\\n95.6%\\n●\\nClass\\nX,\\nKola\\nSaraswathi\\nSchool\\n(2020):\\n95.8%\\nEXPERIENCE\\n●\\nFinmitr\\nOctober ’24-\\nPresent \\nAI-ML\\nHEAD\\n●\\nLeading\\nthe\\ndevelopment\\nof\\n\"FinGuru\",\\nan\\nAI\\nchatbot\\nfor\\nfinancial\\nplanning\\nthat\\nanalyzes\\nuser \\nportfolios,\\nflags\\nfinancial\\nhealth\\nindicators,\\nand\\nprovides\\ntailored\\nadvice\\non\\nmajor\\nfinancial\\ndecisions, \\nexcluding\\nstock\\npicking.\\nThe\\nbot\\nnavigates\\nusers\\nthrough\\napp\\nfeatures\\nbased\\non\\ntheir\\nneeds. \\n●\\nBioscr een\\nPvt\\nLtd\\nAugust’24\\n-\\nPresent \\nAI\\nData\\nIntegration\\nEngineer \\n○\\nCollaborating\\nwith\\na\\ncross-domain\\nteam\\nto\\ndevelop\\na\\ndynamic\\npipeline\\nthat\\nanalyzes\\nscraped\\ndata\\nfor \\nML\\nclassification.\\nCo-developing\\nan\\nAI-driven\\nchatbo\\nt\\non\\na\\ncentralized\\nweb\\nplatform\\n,\\nenabling\\nusers \\nto\\nquery\\ncategorized\\ndata\\nand\\nmake\\ninformed\\npurchasing\\ndecisions.\\nCurrently\\nin\\ndevelopment.\\n●'),\n",
              " Document(id='13a9a89d-32b5-47c1-821d-eeb423aacd07', metadata={}, page_content=\"sensor\\ndata\\nand\\nenabling\\nautonomous\\nnavigation.\\nPROJECTS\\nPDFInsightAI\\nJune’24\\n●\\nDeveloped\\nPDFInsightAI,\\na\\nmultilingual\\nchatbo\\nt\\nutilizing\\nLangchain,\\nHugging\\nFace\\nfor\\nRetrieval\\nAugmented \\nGeneration\\n(RAG)\\n-based\\ncustomized\\nresponses. \\n●\\nIntegrated\\nOptical\\nCharacter\\nRecognition\\n(OCR\\n)\\nusing\\nOpenCV,\\nPIL,\\nand\\nPyTesseract\\nto\\nextract\\ntext\\nfrom \\ndocument\\nimages\\nfor\\ncontext-based\\nanswers. \\n●\\nCreated\\na\\ndata\\npipeline\\nto\\nfeed\\nthe\\nmodel\\nwith\\nthe\\nrequired\\ndataset\\nfor\\nfine-tuning\\n. \\n●\\nImplemented\\na\\nmulti-agent\\narchitecture\\n,\\nwhere\\none\\nagent\\nfunctions\\nas\\nRAG\\nand\\nthe\\nother\\nprovides\\na\\ntool\\nfor \\nvoice\\nsearch\\nin\\nmultiple\\nlanguages\\n. \\n●\\nEnabled\\nthe\\nmodel\\nto\\ngenerate\\nresponses\\nin\\nthe\\nquery's\\nlanguage,\\nwith\\nthe\\nflexibility\\nto\\ncustomize\\nanswers\\nin\\nother \\nlanguages.\\nAdvanced\\nMedical\\nImage\\nSynthesis\\nMarch’24\\n●\\nImplemented\\ndiverse\\ndeep\\nlearning\\nmethods,\\nincluding\\nConvolutional\\nNeural\\nNetworks\\n(CNNs)\\n,\\nautoencoders\\n, \\nand\\nGenerative\\nAdversarial\\nNetworks\\n(GANs)\\n,\\nto\\ncreate\\nlifelike\\nmedical\\nimages. \\n●\\nGenerated\"),\n",
              " Document(id='d72ea8e8-a0b9-4ca0-85e1-dd300c2e1b0f', metadata={}, page_content='Implemented\\ndiverse\\ndeep\\nlearning\\nmethods,\\nincluding\\nConvolutional\\nNeural\\nNetworks\\n(CNNs)\\n,\\nautoencoders\\n, \\nand\\nGenerative\\nAdversarial\\nNetworks\\n(GANs)\\n,\\nto\\ncreate\\nlifelike\\nmedical\\nimages. \\n●\\nGenerated\\nrealistic\\nimages\\nof\\nbrain\\ntumors\\n,\\nchest\\nX-rays\\n,\\nand\\npolyps\\nto\\nsupport\\nmedical\\nresearch\\nand \\ndiagnostics. \\n●\\nEnhanced\\ndiagnostic\\nprecision\\nand\\ncontributed\\nto\\nadvancing\\nmedical\\nresearch\\ninitiatives\\nthrough\\nsynthetic \\nmedical\\nimaging.\\nSKILLS\\nLanguages\\n:\\nPython,\\nJava,\\nC,\\nC++,\\nR,\\nSQL. \\nMachine\\nLearning\\nFrameworks\\n:\\nTensorFlow,\\nPyTorch,\\nKeras. \\nLLM:\\nHugging\\nFace,\\nLangChain,\\nTransformers,\\nLoRA,\\nQLoRA,\\nRAG,\\nLLM\\nAgents,\\nFine\\nTuning. \\nNatural\\nLanguage\\nProcessing\\n:\\nBERT,\\nGPT,\\nLSTM,\\nNLTK,\\nSpaCy. \\nVersion\\nControl\\nand\\nCloud:\\nGit,\\nGitHub,\\nDocker,\\nAWS,\\nMicrosoft\\nAzure.')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 512\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "x4hwDFEVORB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = doc_search.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
        "\n",
        "rqa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "2oErNxoxOcfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_query_rag(query):\n",
        "  answer = rqa.invoke(query)[\"result\"]\n",
        "  return answer"
      ],
      "metadata": {
        "id": "aPXJYwfKOepc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Agent 2 - Web Search"
      ],
      "metadata": {
        "id": "0XsVF_bXOgsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def web_search(query):\n",
        "    print(\"Performing web search using SerpAPI...\")\n",
        "    from google.colab import userdata\n",
        "    serpapi_api_key = userdata.get(\"serpapi_key\")\n",
        "    if not serpapi_api_key:\n",
        "        raise ValueError(\"SerpAPI key not found. Please check Colab secrets.\")\n",
        "\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"api_key\": serpapi_api_key,\n",
        "        \"num\": 1\n",
        "    }\n",
        "    search = GoogleSearch(params)\n",
        "    result = search.get_dict()\n",
        "    if \"organic_results\" in result and len(result[\"organic_results\"]) > 0:\n",
        "        return result[\"organic_results\"][0].get(\"snippet\", \"No snippet found.\")\n",
        "\n",
        "    #if web search fails then we resort to llm answer - useful when the classifier classifies it incorrectly.\n",
        "    return llm_answer_generation(query)"
      ],
      "metadata": {
        "id": "j_kDIGR3OkNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Agent 3 - LLM generation"
      ],
      "metadata": {
        "id": "xLu8UEOEOoc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_answer_generation(question):\n",
        "    print(\"Generating answer using Mistral model...\")\n",
        "    model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "    inputs = tokenizer(f\"Answer the following question concisely: {question}\", return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_length=50)\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()"
      ],
      "metadata": {
        "id": "MwtrC0TEOlib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test 2"
      ],
      "metadata": {
        "id": "Vv3GYx2BOsvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_final_answer(question, classification):\n",
        "    if classification == \"RAG\":\n",
        "        return answer_query_rag(question)\n",
        "    elif classification == \"Web Search\":\n",
        "        return web_search(question)\n",
        "    elif classification == \"LLM Answer\":\n",
        "        return llm_answer_generation(question)\n",
        "    else:\n",
        "        return \"Unable to determine the appropriate action.\""
      ],
      "metadata": {
        "id": "f083OOHxO385"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_test():\n",
        "    print(\"--- Sample Use Case: Full Workflow ---\")\n",
        "\n",
        "    mock_transcription = '''\n",
        "    Hi there, Arun. Hope you are doing well. This is in regard to the internship position that you have applied at our company.\n",
        "    explain more about your advanced image generation project.\n",
        "    '''\n",
        "\n",
        "    # mock_transcription = ''' hi there! how is your day going. this is in regard to the internship posistion that you have applied at our compnay.\n",
        "    # who is the prime minister of india ?'''\n",
        "\n",
        "    print(\"Transcription:\", mock_transcription)\n",
        "    question = extract_question(mock_transcription)\n",
        "    print(\"Extracted Question:\", question)\n",
        "\n",
        "    classification = classify_action(question)\n",
        "    print(\"Action Decided:\", classification)\n",
        "\n",
        "    answer = get_final_answer(question, classification)\n",
        "    print(\"Final Answer:\", answer)"
      ],
      "metadata": {
        "id": "7s-08NDIO5WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "ypC02rKVO7uG",
        "outputId": "366972a9-3c3e-42ec-c24f-c6e82f7d5ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sample Use Case: Full Workflow ---\n",
            "Transcription: \n",
            "    Hi there, Arun. Hope you are doing well. This is in regard to the internship position that you have applied at our company.\n",
            "    explain more about your advanced image generation project.\n",
            "    \n",
            "Extracted Question: \"\"\"\n",
            "    return \"What is your main question about the internship position?\"\n",
            "\n",
            "The main question asked by the user is \"What is your main question about the internship position?\"\n",
            "Classifying question using Zero-Shot Classification model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Decided: Web Search\n",
            "Performing web search using SerpAPI...\n",
            "Generating answer using Mistral model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1.\n403 Client Error. (Request ID: Root=1-676469c2-63b4337d2ace4cc069673694;55bfac7e-d749-4532-9465-fd9df5f484c0)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 to ask for access.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    422\u001b[0m             )\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-676469c2-63b4337d2ace4cc069673694;55bfac7e-d749-4532-9465-fd9df5f484c0)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 to ask for access.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-9562cb93d925>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-bf2fbbb7f5b2>\u001b[0m in \u001b[0;36msample_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Action Decided:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_final_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Answer:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-24f3a9a2ac1a>\u001b[0m in \u001b[0;36mget_final_answer\u001b[0;34m(question, classification)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0manswer_query_rag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Web Search\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweb_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LLM Answer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mllm_answer_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-b4872b000cbb>\u001b[0m in \u001b[0;36mweb_search\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#if web search fails then we resort to llm answer - useful when the classifier classifies it incorrectly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mllm_answer_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-8cf62192e65b>\u001b[0m in \u001b[0;36mllm_answer_generation\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating answer using Mistral model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mistralai/Mistral-7B-Instruct-v0.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    879\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    650\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1.\n403 Client Error. (Request ID: Root=1-676469c2-63b4337d2ace4cc069673694;55bfac7e-d749-4532-9465-fd9df5f484c0)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 to ask for access."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pipeline Integeration of Agents"
      ],
      "metadata": {
        "id": "0G4qHOx0PEIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pipeline(audio_path_1, audio_path_2):\n",
        "    transcription_1 = transcribe_audio_with_groq(audio_path_1)\n",
        "    transcription_2 = transcribe_audio_with_groq(audio_path_2)\n",
        "\n",
        "    combined_transcription = transcription_1 + \" \" + transcription_2\n",
        "\n",
        "    question = extract_question(combined_transcription)\n",
        "\n",
        "    classification = classify_action(question)\n",
        "\n",
        "    if classification == \"RAG\":\n",
        "        answer = answer_query_rag(question)\n",
        "    elif classification == \"Web Search\":\n",
        "        answer = web_search(question)\n",
        "    else:\n",
        "        answer = llm_answer_generation(question)\n",
        "\n",
        "    return combined_transcription, question, classification, answer"
      ],
      "metadata": {
        "id": "gXp4OOdwPONZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_interface(audio_file_person1, audio_file_person2):\n",
        "    transcription, question, classification, answer = process_pipeline(audio_file_person1, audio_file_person2)\n",
        "    return transcription, question, classification, answer"
      ],
      "metadata": {
        "id": "fcLVI0V3PSGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as ui:\n",
        "    gr.Markdown(\"# Speech Query Processing System\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            person1_audio = gr.Audio(type=\"filepath\", label=\"Person 1 Audio Input\")\n",
        "            person2_audio = gr.Audio(type=\"filepath\", label=\"Person 2 Audio Input\")\n",
        "            submit_button = gr.Button(\"Help Me\")\n",
        "            clear_button = gr.Button(\"Clear\")\n",
        "\n",
        "        with gr.Column():\n",
        "            transcription_box = gr.Textbox(label=\"Transcription\")\n",
        "            question_box = gr.Textbox(label=\"Extracted Question\")\n",
        "            classification_box = gr.Textbox(label=\"Classification\")\n",
        "            answer_box = gr.Textbox(label=\"Final Answer\")\n",
        "\n",
        "    def gradio_interface(audio_file_person1, audio_file_person2):\n",
        "        transcription, question, classification, answer = process_pipeline(audio_file_person1, audio_file_person2)\n",
        "        return transcription, question, classification, answer\n",
        "\n",
        "    submit_button.click(\n",
        "        fn=gradio_interface,\n",
        "        inputs=[person1_audio, person2_audio],\n",
        "        outputs=[transcription_box, question_box, classification_box, answer_box]\n",
        "    )\n",
        "\n",
        "    clear_button.click(\n",
        "        fn=lambda: (\"\", \"\", \"\", \"\"),\n",
        "        inputs=[],\n",
        "        outputs=[transcription_box, question_box, classification_box, answer_box]\n",
        "    )"
      ],
      "metadata": {
        "id": "5BfunA-fPeN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ui.launch()"
      ],
      "metadata": {
        "id": "d9F0WHy7PiM5",
        "outputId": "b82a531e-a035-49e7-f155-f430a4536a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7f97e58293672635fc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7f97e58293672635fc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0YB0Y3GPjpn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
